# Weekly Incident Review - Week 50, 2024

**Date:** December 9-13, 2024
**Facilitator:** Sarah Chen (SRE Lead)
**Attendees:** Platform Team, Payments Team, Identity Team

---

## Executive Summary

This week we had 3 incidents: one P1 affecting payment processing, one P2 related to order service memory issues, and one P3 involving slow API responses during a traffic spike. Total customer impact was approximately 45 minutes of degraded service.

---

## Incident 1: Payment Database Connection Pool Exhaustion

**Incident ID:** INC-2024-1209-001
**Severity:** P1
**Duration:** 25 minutes (14:05 - 14:30 UTC)
**Services Affected:** payment-service, checkout-api

### Timeline
- 14:00 - Customer reports of failed payments begin appearing in support queue
- 14:05 - On-call paged by customer support (manual escalation)
- 14:12 - Root cause identified: payment-service Postgres connection pool at 100%
- 14:20 - Temporary mitigation: increased connection pool from 20 to 50
- 14:30 - Service fully restored

### Root Cause
The payment-service database connection pool was configured with a maximum of 20 connections. During a promotional campaign, transaction volume increased 3x, exhausting the pool. New payment requests received 503 errors.

### Impact
- ~1,200 failed payment attempts
- Estimated revenue impact: $85,000
- Customer trust impact: High (payment failures during checkout)

### What Went Well
- Quick identification of root cause once engineers engaged
- Mitigation was straightforward

### What Went Wrong
- **No alert for connection pool utilization** - we only found out via customer complaints
- Detection took 5+ minutes after the issue started
- On-call was paged by support, not automated alerting

### Action Items

#### Alert Improvements Needed

1. **Add alert for payment-service database connection pool utilization**
   - Alert when connection pool usage exceeds 70% for more than 2 minutes
   - Service: payment-service
   - Severity: warning
   - Team: payments
   - Rationale: Early warning before pool exhaustion

2. **Add critical alert for payment-service database connection pool near exhaustion**
   - Alert when connection pool usage exceeds 90% for more than 30 seconds
   - Service: payment-service
   - Severity: critical
   - Team: payments
   - Should page on-call immediately

3. **Add alert for payment-service 503 error spike**
   - Alert when 503 error rate exceeds 0.5% for more than 1 minute
   - Service: payment-service
   - Severity: critical
   - Team: payments
   - Catches service unavailability quickly

---

## Incident 2: Order Service Memory Leak

**Incident ID:** INC-2024-1211-001
**Severity:** P2
**Duration:** 3 hours (02:00 - 05:00 UTC)
**Services Affected:** order-service

### Timeline
- 02:00 - Order service pods begin OOM-killing and restarting
- 02:15 - PagerDuty alert fires for pod restarts (existing alert)
- 02:30 - On-call investigates, sees memory growing linearly
- 03:00 - Root cause identified: memory leak in order caching
- 04:30 - Hotfix deployed disabling the problematic cache
- 05:00 - Memory stable, incident resolved

### Root Cause
A recent change to the order caching logic introduced a memory leak. Each order lookup added to an in-memory cache but never evicted entries. Memory grew at ~500MB/hour until pods OOM'd.

### Impact
- Intermittent order lookup failures during pod restarts
- No data loss
- Order confirmation emails delayed by up to 10 minutes

### What Went Well
- Existing pod restart alert worked correctly
- On-call responded quickly despite 2am page

### What Went Wrong
- **No alert for abnormal memory growth rate** - only caught after OOM
- Took 30 minutes to correlate pod restarts with memory issue
- No visibility into per-service memory trends

### Action Items

#### Alert Improvements Needed

4. **Add alert for order-service memory growth rate**
   - Alert when container memory usage grows more than 100MB in 15 minutes
   - Service: order-service
   - Severity: warning
   - Team: orders
   - Catches memory leaks before OOM

5. **Add alert for order-service high memory utilization**
   - Alert when container memory usage exceeds 80% of limit for more than 5 minutes
   - Service: order-service
   - Severity: warning
   - Team: orders
   - Pre-OOM warning

6. **Add alert for order-service OOMKilled events**
   - Alert immediately when any order-service pod is OOMKilled
   - Service: order-service
   - Severity: critical
   - Team: orders
   - Faster detection than restart count

---

## Incident 3: API Gateway Latency During Traffic Spike

**Incident ID:** INC-2024-1213-001
**Severity:** P3
**Duration:** 40 minutes (10:30 - 11:10 UTC)
**Services Affected:** api-gateway

### Timeline
- 10:15 - Marketing campaign email sent to 500k customers
- 10:30 - API latency begins increasing
- 10:45 - Customer complaints about slow website
- 10:50 - Platform team notified via Slack
- 10:55 - HPA scaling kicks in, new pods spinning up
- 11:10 - Latency returns to normal

### Root Cause
Marketing campaign drove unexpected traffic spike. The HPA (Horizontal Pod Autoscaler) was configured with high scale-up thresholds, so it took 25 minutes to add sufficient capacity.

### Impact
- P95 latency increased from 200ms to 2.5s
- Some users experienced timeouts
- Bounce rate increased 15% during incident

### What Went Well
- HPA eventually scaled correctly
- No service outage, just degradation

### What Went Wrong
- **Existing latency alert (P95 > 500ms) took 5 minutes to fire** - by then users were complaining
- No alert for sudden traffic spikes before latency degraded
- No coordination between marketing and platform teams

### Action Items

#### Alert Improvements Needed

7. **Add alert for API Gateway sudden traffic increase**
   - Alert when request rate increases more than 50% compared to previous hour
   - Service: api-gateway
   - Severity: warning
   - Team: platform
   - Early warning before latency impact

8. **Add alert for API Gateway request queue depth**
   - Alert when pending request queue exceeds 1000 for more than 1 minute
   - Service: api-gateway
   - Severity: warning
   - Team: platform
   - Indicates capacity stress before latency degrades

9. **Reduce latency alert threshold timing**
   - Current: P95 > 500ms for 5 minutes
   - New: P95 > 500ms for 2 minutes
   - Service: api-gateway
   - Severity: warning
   - Team: platform
   - Faster detection of latency issues

---

## Summary of Alert Improvements

| # | Alert | Service | Severity | Team | Priority |
|---|-------|---------|----------|------|----------|
| 1 | Connection pool > 70% | payment-service | warning | payments | P1 |
| 2 | Connection pool > 90% | payment-service | critical | payments | P1 |
| 3 | 503 error rate > 0.5% | payment-service | critical | payments | P1 |
| 4 | Memory growth > 100MB/15min | order-service | warning | orders | P2 |
| 5 | Memory > 80% limit | order-service | warning | orders | P2 |
| 6 | OOMKilled events | order-service | critical | orders | P2 |
| 7 | Traffic increase > 50% | api-gateway | warning | platform | P2 |
| 8 | Request queue > 1000 | api-gateway | warning | platform | P2 |
| 9 | Reduce latency alert timing | api-gateway | warning | platform | P3 |

---

## Next Steps

1. **Payments Team:** Implement alerts 1-3 by end of week
2. **Orders Team:** Implement alerts 4-6 by end of week
3. **Platform Team:** Implement alerts 7-9 and reduce latency threshold
4. **All Teams:** Review HPA configurations for faster scale-up
5. **Process:** Establish marketing campaign notification process

---

**Next Review:** December 20, 2024


